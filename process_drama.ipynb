{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd13d49b",
   "metadata": {},
   "source": [
    "# üé≠ Drama Data Processor\n",
    "\n",
    "Notebook n√†y d√πng ƒë·ªÉ:\n",
    "- Chu·∫©n h√≥a d·ªØ li·ªáu th√¥ t·ª´ c√°c b√†i vi·∫øt m·∫°ng x√£ h·ªôi (JSON)\n",
    "- L·ªçc b√¨nh lu·∫≠n spam\n",
    "- G√°n nh√£n d∆∞ lu·∫≠n b·∫±ng zero-shot classification\n",
    "- T√≥m t·∫Øt c√°c nh√≥m √Ω ki·∫øn ch√≠nh\n",
    "\n",
    "üëâ S·ª≠ d·ª•ng m√¥ h√¨nh `facebook/bart-large-mnli` t·ª´ HuggingFace Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431589e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d20f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_post = \"data_fb.json\"\n",
    "path_check = \"links_fb.json\"\n",
    "path_data_out = \"clean_data.json\"\n",
    "path_static_out = \"static_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dcae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(path_post, 'r') as f:\n",
    "    data_post = json.load(f)\n",
    "\n",
    "with open(path_check, 'r') as f:\n",
    "    data_check = json.load(f)\n",
    "\n",
    "print(\"S·ªë b√†i post:\", len(data_post))\n",
    "print(\"S·ªë entry trong check_link:\", len(data_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file check_link\n",
    "with open(path_check, \"r\") as f:\n",
    "    check_data = json.load(f)\n",
    "\n",
    "check_time_map = {item[\"source_url\"]: item[\"time\"] for item in check_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot model\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Nh√£n d∆∞ lu·∫≠n ch√≠nh\n",
    "labels = [\"ƒê·ªìng t√¨nh\", \"Ch·ªâ tr√≠ch\", \"Ch·∫ø gi·ªÖu\", \"·ª¶ng h·ªô\", \"Ph·∫£n ƒë·ªëi\", \"Trung l·∫≠p\", \"Th√¥ng c·∫£m\"]\n",
    "\n",
    "# H√†m chu·∫©n h√≥a th·ªùi gian\n",
    "def normalize_time(post_url, raw_time):\n",
    "    try:\n",
    "        return check_time_map.get(post_url, raw_time)\n",
    "    except:\n",
    "        return raw_time\n",
    "\n",
    "# Load post g·ªëc\n",
    "with open(path_post, \"r\") as f:\n",
    "    posts = json.load(f)\n",
    "\n",
    "clean_data = []\n",
    "static_data = []\n",
    "\n",
    "for post in tqdm(posts, desc=\"ƒêang x·ª≠ l√Ω b√†i vi·∫øt\"):\n",
    "    url = post.get(\"url\", \"\")\n",
    "    content = post.get(\"content\", \"\")\n",
    "    poster = post.get(\"poster\", \"\")\n",
    "    time = normalize_time(url, post.get(\"time\", \"\"))\n",
    "    comments = post.get(\"comments\", [])\n",
    "\n",
    "    opinion_summary = {}\n",
    "    opinion_examples = {}\n",
    "\n",
    "    for cmt in comments:\n",
    "        text = cmt.get(\"text\", \"\").strip()\n",
    "        if not text or len(text) < 6: continue\n",
    "        result = classifier(text, labels)\n",
    "        best_label = result[\"labels\"][0]\n",
    "        score = result[\"scores\"][0]\n",
    "\n",
    "        if best_label not in opinion_summary:\n",
    "            opinion_summary[best_label] = 0\n",
    "            opinion_examples[best_label] = []\n",
    "\n",
    "        opinion_summary[best_label] += 1\n",
    "        if len(opinion_examples[best_label]) < 3:\n",
    "            opinion_examples[best_label].append(text)\n",
    "\n",
    "    # Cleaned data (cho LLM d√πng)\n",
    "    clean_post = {\n",
    "        \"url\": url,\n",
    "        \"poster\": poster,\n",
    "        \"time\": time,\n",
    "        \"content\": content,\n",
    "        \"summary\": {\n",
    "            \"num_opinions\": len(opinion_summary),\n",
    "            \"opinions\": [\n",
    "                {\n",
    "                    \"label\": label,\n",
    "                    \"description\": f\"S·ªë l∆∞·ª£ng comment: {count}\",\n",
    "                    \"example_comments\": opinion_examples[label]\n",
    "                } for label, count in opinion_summary.items()\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    clean_data.append(clean_post)\n",
    "\n",
    "    # Static data (cho th·ªëng k√™)\n",
    "    static_data.append({\n",
    "        \"url\": url,\n",
    "        \"content\": content,\n",
    "        \"summary_stat\": opinion_summary\n",
    "    })\n",
    "\n",
    "# L∆∞u file k·∫øt qu·∫£\n",
    "with open(path_data_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(clean_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(path_static_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(static_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ Ho√†n t·∫•t x·ª≠ l√Ω v√† l∆∞u d·ªØ li·ªáu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03931ec2",
   "metadata": {},
   "source": [
    "‚úÖ **X·ª≠ l√Ω xong!**\n",
    "- D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i `clean_data.json`\n",
    "- B·∫°n c√≥ th·ªÉ ti·∫øp t·ª•c g·ªôp nhi·ªÅu file l·∫°i n·∫øu c·∫ßn\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
