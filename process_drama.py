!pip install -q transformers accelerate tqdm
import torch
print(torch.cuda.get_device_name(0))
import os
import json
from tqdm import tqdm
from transformers import pipeline

# Zero-shot classifier
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
# File ƒë·∫ßu v√†o
INPUT_FILE = "Thinh_all_posts.json"
CHECK_FILE = "links_fb.json"
OUTPUT_FILE = "processed_data.json"


from datetime import datetime
import re

def merge_author(poster, author):
    return author if author else poster

def normalize_time(raw_time, url):
    # V√≠ d·ª•: "28 th√°ng 6 l√∫c 11:25" => "2025-06-28T11:25"
    try:
        pattern = r"(\d{1,2}) th√°ng (\d{1,2}) l√∫c (\d{1,2}):(\d{2})"
        match = re.search(pattern, raw_time)
        if match:
            day, month, hour, minute = map(int, match.groups())
            dt = datetime(2025, month, day, hour, minute)
            return dt.isoformat()
    except Exception as e:
        print(f"[!] L·ªói parse time: {raw_time} ({url}) ‚Üí {e}")
    return raw_time

def is_valid_comment(text):
    if not text.strip():
        return False
    if len(text.strip()) < 3:
        return False
    if re.fullmatch(r"[\W_]+", text):  # icon-only
        return False
    if re.search(r"https?://", text):
        return False
    return True

def classify_comment(text):
    labels = ["T√≠ch c·ª±c", "Ti√™u c·ª±c", "Trung l·∫≠p"]
    result = classifier(text, labels)
    return result["labels"][0], float(result["scores"][0])

# Load d·ªØ li·ªáu g·ªëc
with open(INPUT_FILE, 'r', encoding='utf-8') as f:
    data = json.load(f)

# Load check_link
with open(CHECK_FILE, 'r', encoding='utf-8') as f:
    raw_check_data = json.load(f)

# T·∫°o dict check_data
check_data = {item["source_url"]: item["time"] for item in raw_check_data}

# N·∫øu OUTPUT_FILE ƒë√£ t·ªìn t·∫°i, load d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
if os.path.exists(OUTPUT_FILE):
    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
        processed = json.load(f)
    print(f"‚úÖ ƒê√£ load {len(processed)} b√†i ƒë√£ x·ª≠ l√Ω t·ª´ '{OUTPUT_FILE}'.")
else:
    processed = []
    print("üîµ Ch∆∞a c√≥ d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω, b·∫Øt ƒë·∫ßu m·ªõi.")

# T·∫°o set ƒë·ªÉ tr√°nh x·ª≠ l√Ω tr√πng
processed_urls = {post["url"] for post in processed}
# B·∫Øt ƒë·∫ßu x·ª≠ l√Ω
total_posts = len(data)
done_count = len(processed)

progress = tqdm(data, desc=f"Processing posts ({done_count}/{total_posts})")

for post in progress:
    url = post.get("url", "")
    if url in processed_urls:
        progress.set_postfix_str("ƒê√£ x·ª≠ l√Ω, b·ªè qua")
        continue

    # G·ªôp author
    post["author"] = merge_author(post.get("poster", ""), post.get("author", ""))
    post["time"] = normalize_time(post.get("time", ""), url)

    valid_comments = []
    opinion_summary = {}

    for cmt in post.get("comments", []):
        if not is_valid_comment(cmt["text"]):
            continue
        label, score = classify_comment(cmt["text"])
        cmt["label"] = label
        cmt["score"] = round(score, 3)

        if label not in opinion_summary:
            opinion_summary[label] = []
        opinion_summary[label].append(cmt["text"])

        valid_comments.append(cmt)

    post["comments"] = valid_comments
    post["opinion_summary"] = {k: v[:3] for k, v in opinion_summary.items()}

    processed.append(post)
    processed_urls.add(url)

    # Save ngay sau m·ªói b√†i
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as out:
        json.dump(processed, out, ensure_ascii=False, indent=2)

    done_count += 1
    progress.set_postfix_str(f"ƒê√£ x·ª≠ l√Ω {done_count}/{total_posts}")

print("‚úÖ Ho√†n t·∫•t x·ª≠ l√Ω t·∫•t c·∫£ b√†i.")

